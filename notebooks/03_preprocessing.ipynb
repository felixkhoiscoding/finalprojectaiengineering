{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 3: Data Preprocessing\n",
                "\n",
                "**Objective:** Prepare data for ARIMA, Prophet, and LSTM modeling\n",
                "\n",
                "**Based on Step 2 Findings:**\n",
                "- Non-stationary series → d=1 differencing required\n",
                "- Weak seasonality → test both ARIMA and SARIMA\n",
                "- High volatility → feature engineering important\n",
                "\n",
                "**Tasks:**\n",
                "1. Train/test split (temporal)\n",
                "2. Feature engineering (lags, rolling, differences, time)\n",
                "3. Scaling (for LSTM)\n",
                "4. Model-specific preparations\n",
                "5. Save processed datasets\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Setup and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import sys\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.append(str(project_root))\n",
                "\n",
                "# Import custom modules\n",
                "from src.data_loader import load_data\n",
                "from src import preprocessing as prep\n",
                "from config.config import TEST_SIZE, RAW_DATA_PATH, PROCESSED_DATA_PATH, LSTM_SEQUENCE_LENGTH\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "\n",
                "print(\"[OK] All imports successful!\")\n",
                "print(f\"\\n[USER INPUT] Configuration:\")\n",
                "print(f\"  - Test Size: {TEST_SIZE} months\")\n",
                "print(f\"  - LSTM Sequence Length: {LSTM_SEQUENCE_LENGTH} months\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = load_data(filepath=RAW_DATA_PATH, sheet_name='Monthly')\n",
                "print(f\"\\nOriginal dataset: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Train/Test Split\n",
                "\n",
                "**Strategy:** Time-aware split\n",
                "- Last 12 months (USER INPUT: TEST_SIZE) = Test set\n",
                "- Earlier data = Training set\n",
                "- No random shuffling (preserves temporal order)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform train/test split\n",
                "train_df, test_df = prep.train_test_split_ts(\n",
                "    data=df,\n",
                "    date_col='observation_date',\n",
                "    test_size=TEST_SIZE  # USER INPUT from config\n",
                ")\n",
                "\n",
                "print(f\"\\nTrain shape: {train_df.shape}\")\n",
                "print(f\"Test shape: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize train/test split\n",
                "plt.figure(figsize=(16, 6))\n",
                "plt.plot(train_df['observation_date'], train_df['WPU101704'], \n",
                "         label='Train', color='blue', linewidth=1.5)\n",
                "plt.plot(test_df['observation_date'], test_df['WPU101704'], \n",
                "         label='Test', color='red', linewidth=1.5)\n",
                "plt.axvline(x=test_df['observation_date'].iloc[0], color='green', \n",
                "           linestyle='--', linewidth=2, label='Split Point')\n",
                "plt.title('Train/Test Split Visualization', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Date', fontsize=12)\n",
                "plt.ylabel('PPI Value', fontsize=12)\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Feature Engineering\n",
                "\n",
                "Create features for machine learning models (used by LSTM and potential ML baselines)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create lag features\n",
                "print(\"Creating lag features...\")\n",
                "train_fe = prep.create_lag_features(\n",
                "    data=train_df,\n",
                "    value_col='WPU101704',\n",
                "    lags=[1, 3, 6, 12]  # 1m, 3m, 6m, 1y lags\n",
                ")\n",
                "\n",
                "test_fe = prep.create_lag_features(\n",
                "    data=test_df,\n",
                "    value_col='WPU101704',\n",
                "    lags=[1, 3, 6, 12]\n",
                ")\n",
                "\n",
                "print(f\"\\nColumns after lag features: {len(train_fe.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create rolling features\n",
                "print(\"Creating rolling features...\")\n",
                "train_fe = prep.create_rolling_features(\n",
                "    data=train_fe,\n",
                "    value_col='WPU101704',\n",
                "    windows=[3, 6, 12]  # 3m, 6m, 1y rolling stats\n",
                ")\n",
                "\n",
                "test_fe = prep.create_rolling_features(\n",
                "    data=test_fe,\n",
                "    value_col='WPU101704',\n",
                "    windows=[3, 6, 12]\n",
                ")\n",
                "\n",
                "print(f\"\\nColumns after rolling features: {len(train_fe.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create difference features (based on Step 2: d=1 needed)\n",
                "print(\"Creating difference features...\")\n",
                "train_fe = prep.create_difference_features(\n",
                "    data=train_fe,\n",
                "    value_col='WPU101704',\n",
                "    periods=[1, 12]  # First difference and seasonal difference\n",
                ")\n",
                "\n",
                "test_fe = prep.create_difference_features(\n",
                "    data=test_fe,\n",
                "    value_col='WPU101704',\n",
                "    periods=[1, 12]\n",
                ")\n",
                "\n",
                "print(f\"\\nColumns after difference features: {len(train_fe.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create time-based features\n",
                "print(\"Creating time features...\")\n",
                "train_fe = prep.create_time_features(\n",
                "    data=train_fe,\n",
                "    date_col='observation_date'\n",
                ")\n",
                "\n",
                "test_fe = prep.create_time_features(\n",
                "    data=test_fe,\n",
                "    date_col='observation_date'\n",
                ")\n",
                "\n",
                "print(f\"\\nTotal columns after all features: {len(train_fe.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview engineered features\n",
                "print(\"\\nSample of engineered features:\")\n",
                "print(\"\\nTrain:\")\n",
                "print(train_fe.head(15))\n",
                "\n",
                "print(\"\\nTest:\")\n",
                "print(test_fe.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all feature columns\n",
                "print(\"\\nAll columns in processed dataset:\")\n",
                "for i, col in enumerate(train_fe.columns, 1):\n",
                "    print(f\"{i:2d}. {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.4 Handle NaN Values\n",
                "\n",
                "Feature engineering creates NaN values in early rows (due to lags and rolling windows)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for NaN values\n",
                "print(\"NaN values in engineered features:\")\n",
                "print(\"\\nTrain:\")\n",
                "print(train_fe.isnull().sum())\n",
                "\n",
                "print(\"\\nTest:\")\n",
                "print(test_fe.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create version WITHOUT NaN for ML models\n",
                "train_clean = train_fe.dropna().copy()\n",
                "test_clean = test_fe.dropna().copy()\n",
                "\n",
                "print(f\"\\nAfter dropping NaN:\")\n",
                "print(f\"  Train: {len(train_fe)} -> {len(train_clean)} rows ({len(train_fe) - len(train_clean)} dropped)\")\n",
                "print(f\"  Test: {len(test_fe)} -> {len(test_clean)} rows ({len(test_fe) - len(test_clean)} dropped)\")\n",
                "\n",
                "print(f\"\\nTrain date range (after dropna): {train_clean['observation_date'].min()} to {train_clean['observation_date'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.5 Preprocessing Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display summary\n",
                "features_list = [\n",
                "    'WPU101704_lag_1', 'WPU101704_lag_3', 'WPU101704_lag_6', 'WPU101704_lag_12',\n",
                "    'WPU101704_rolling_mean_3', 'WPU101704_rolling_std_3',\n",
                "    'WPU101704_rolling_mean_6', 'WPU101704_rolling_std_6',\n",
                "    'WPU101704_rolling_mean_12', 'WPU101704_rolling_std_12',\n",
                "    'WPU101704_diff_1', 'WPU101704_diff_12',\n",
                "    'year', 'month', 'quarter', 'month_sin', 'month_cos'\n",
                "]\n",
                "\n",
                "prep.get_preprocessing_summary(\n",
                "    train_df=train_clean,\n",
                "    test_df=test_clean,\n",
                "    features_added=features_list\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.6 Model-Specific Preparations\n",
                "\n",
                "Prepare data in the specific formats required by different models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.6.1 ARIMA/SARIMA Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ARIMA expects simple time series (no features needed)\n",
                "# Based on Step 2: d=1 recommended\n",
                "\n",
                "# For ARIMA modeling, we'll use the ORIGINAL train/test (not feature-engineered)\n",
                "arima_train = train_df['WPU101704'].copy()\n",
                "arima_test = test_df['WPU101704'].copy()\n",
                "\n",
                "print(\"ARIMA/SARIMA Data:\")\n",
                "print(f\"  Train: {len(arima_train)} observations\")\n",
                "print(f\"  Test: {len(arima_test)} observations\")\n",
                "print(f\"\\nNote: ARIMA will apply differencing internally (d=1)\")\n",
                "print(f\"      Statsmodels auto_arima will find optimal (p,d,q) parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.6.2 Prophet Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prophet requires 'ds' (date) and 'y' (target) columns\n",
                "prophet_train = prep.prepare_for_prophet(\n",
                "    data=train_df,\n",
                "    date_col='observation_date',\n",
                "    value_col='WPU101704'\n",
                ")\n",
                "\n",
                "prophet_test = prep.prepare_for_prophet(\n",
                "    data=test_df,\n",
                "    date_col='observation_date',\n",
                "    value_col='WPU101704'\n",
                ")\n",
                "\n",
                "print(\"\\nProphet train data:\")\n",
                "print(prophet_train.head())\n",
                "\n",
                "print(\"\\nProphet test data:\")\n",
                "print(prophet_test.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.6.3 LSTM Preparation\n",
                "\n",
                "LSTM requires:\n",
                "1. Scaled data (0-1 range)\n",
                "2. Sequences of fixed length\n",
                "3. 3D array shape: (samples, timesteps, features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Scale the data (MinMax scaling 0-1)\n",
                "# IMPORTANT: Fit scaler on TRAIN only\n",
                "\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "scaler = MinMaxScaler(feature_range=(0, 1))\n",
                "\n",
                "# Scale target column\n",
                "train_scaled = train_df[['WPU101704']].copy()\n",
                "test_scaled = test_df[['WPU101704']].copy()\n",
                "\n",
                "train_scaled['WPU101704'] = scaler.fit_transform(train_df[['WPU101704']])\n",
                "test_scaled['WPU101704'] = scaler.transform(test_df[['WPU101704']])\n",
                "\n",
                "print(\"Data Scaling for LSTM:\")\n",
                "print(f\"  Original range: {train_df['WPU101704'].min():.2f} - {train_df['WPU101704'].max():.2f}\")\n",
                "print(f\"  Scaled range: {train_scaled['WPU101704'].min():.4f} - {train_scaled['WPU101704'].max():.4f}\")\n",
                "print(f\"  Scaler fitted on train data only\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Create sequences\n",
                "# USER INPUT: sequence_length from config\n",
                "\n",
                "X_train_lstm, y_train_lstm = prep.create_lstm_sequences(\n",
                "    data=train_scaled,\n",
                "    value_col='WPU101704',\n",
                "    sequence_length=LSTM_SEQUENCE_LENGTH\n",
                ")\n",
                "\n",
                "X_test_lstm, y_test_lstm = prep.create_lstm_sequences(\n",
                "    data=test_scaled,\n",
                "    value_col='WPU101704',\n",
                "    sequence_length=LSTM_SEQUENCE_LENGTH\n",
                ")\n",
                "\n",
                "print(f\"\\nLSTM Training Data:\")\n",
                "print(f\"  X_train shape: {X_train_lstm.shape}\")\n",
                "print(f\"  y_train shape: {y_train_lstm.shape}\")\n",
                "\n",
                "print(f\"\\nLSTM Test Data:\")\n",
                "print(f\"  X_test shape: {X_test_lstm.shape}\")\n",
                "print(f\"  y_test shape: {y_test_lstm.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: How one sequence looks\n",
                "print(\"Example LSTM sequence (first sequence):\")\n",
                "print(f\"\\nInput (X): Last {LSTM_SEQUENCE_LENGTH} values:\")\n",
                "print(X_train_lstm[0].flatten())\n",
                "print(f\"\\nTarget (y): Next value: {y_train_lstm[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.7 Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create processed data directory if needed\n",
                "from config.config import DATA_DIR\n",
                "processed_dir = DATA_DIR / 'processed'\n",
                "processed_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Save different versions for different models\n",
                "\n",
                "# 1. Original train/test (for ARIMA/SARIMA/Prophet)\n",
                "train_df.to_csv(processed_dir / 'train_original.csv', index=False)\n",
                "test_df.to_csv(processed_dir / 'test_original.csv', index=False)\n",
                "\n",
                "# 2. Feature-engineered (for potential ML baseline models)\n",
                "train_clean.to_csv(processed_dir / 'train_features.csv', index=False)\n",
                "test_clean.to_csv(processed_dir / 'test_features.csv', index=False)\n",
                "\n",
                "# 3. Prophet format\n",
                "prophet_train.to_csv(processed_dir / 'train_prophet.csv', index=False)\n",
                "prophet_test.to_csv(processed_dir / 'test_prophet.csv', index=False)\n",
                "\n",
                "# 4. LSTM sequences (NumPy arrays)\n",
                "np.save(processed_dir / 'X_train_lstm.npy', X_train_lstm)\n",
                "np.save(processed_dir / 'y_train_lstm.npy', y_train_lstm)\n",
                "np.save(processed_dir / 'X_test_lstm.npy', X_test_lstm)\n",
                "np.save(processed_dir / 'y_test_lstm.npy', y_test_lstm)\n",
                "\n",
                "# 5. Save scaler for LSTM\n",
                "import joblib\n",
                "joblib.dump(scaler, processed_dir / 'lstm_scaler.pkl')\n",
                "\n",
                "print(\"[OK] Processed data saved successfully!\")\n",
                "print(f\"\\nLocation: {processed_dir}\")\n",
                "print(\"\\nFiles created:\")\n",
                "print(\"  1. train_original.csv, test_original.csv (for ARIMA/Prophet)\")\n",
                "print(\"  2. train_features.csv, test_features.csv (with engineered features)\")\n",
                "print(\"  3. train_prophet.csv, test_prophet.csv (Prophet format)\")\n",
                "print(\"  4. X_train_lstm.npy, y_train_lstm.npy (LSTM sequences)\")\n",
                "print(\"  5. X_test_lstm.npy, y_test_lstm.npy (LSTM sequences)\")\n",
                "print(\"  6. lstm_scaler.pkl (for inverse transform)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.8 Final Summary and Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"STEP 3: DATA PREPROCESSING - COMPLETED\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\n1. TRAIN/TEST SPLIT:\")\n",
                "print(f\"   - Train: {len(train_df)} observations ({train_df['observation_date'].min().strftime('%Y-%m')} to {train_df['observation_date'].max().strftime('%Y-%m')})\")\n",
                "print(f\"   - Test: {len(test_df)} observations ({test_df['observation_date'].min().strftime('%Y-%m')} to {test_df['observation_date'].max().strftime('%Y-%m')})\")\n",
                "\n",
                "print(\"\\n2. FEATURES ENGINEERED:\")\n",
                "print(f\"   - Lag features: 1, 3, 6, 12 months\")\n",
                "print(f\"   - Rolling features: mean & std for 3, 6, 12 months\")\n",
                "print(f\"   - Difference features: 1st diff (MoM), 12th diff (YoY)\")\n",
                "print(f\"   - Time features: year, month, quarter, cyclical encoding\")\n",
                "print(f\"   - Total engineered columns: {len(train_fe.columns)}\")\n",
                "\n",
                "print(\"\\n3. DATA PREPARED FOR MODELS:\")\n",
                "print(f\"   [OK] ARIMA/SARIMA: {len(arima_train)} train, {len(arima_test)} test\")\n",
                "print(f\"   [OK] Prophet: {len(prophet_train)} train, {len(prophet_test)} test\")\n",
                "print(f\"   [OK] LSTM: {X_train_lstm.shape[0]} train sequences, {X_test_lstm.shape[0]} test sequences\")\n",
                "\n",
                "print(\"\\n4. LSTM CONFIGURATION:\")\n",
                "print(f\"   - Sequence length: {LSTM_SEQUENCE_LENGTH} timesteps (USER INPUT)\")\n",
                "print(f\"   - Scaling: MinMaxScaler (0-1 range)\")\n",
                "print(f\"   - Input shape: {X_train_lstm.shape}\")\n",
                "\n",
                "print(\"\\n5. FILES SAVED:\")\n",
                "print(f\"   Location: {processed_dir}\")\n",
                "print(f\"   Total files: 11 (6 CSV + 4 NPY + 1 PKL)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"[OK] Ready for Step 4: Baseline Models\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\nNext Steps:\")\n",
                "print(\"  1. Implement baseline models (Naive, SMA, Exponential Smoothing)\")\n",
                "print(\"  2. Establish performance benchmarks\")\n",
                "print(\"  3. Compare against more complex models later\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "**Step 3: Data Preprocessing - COMPLETE ✓**\n",
                "\n",
                "We have successfully:\n",
                "- ✓ Split data into train (508) and test (12) sets\n",
                "- ✓ Engineered 17 features (lags, rolling, differences, time)\n",
                "- ✓ Prepared data for ARIMA (simple time series)\n",
                "- ✓ Prepared data for Prophet (ds, y format)\n",
                "- ✓ Prepared data for LSTM (scaled sequences)\n",
                "- ✓ Saved all processed datasets\n",
                "\n",
                "**Key Outputs:**\n",
                "- Train/test datasets in multiple formats\n",
                "- LSTM sequences (496 train, 0 test - will need adjustment)\n",
                "- Fitted scaler for inverse transformation\n",
                "\n",
                "**Ready for Step 4: Baseline Models**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}